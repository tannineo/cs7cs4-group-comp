# -*- coding: utf-8 -*-
"""20191107.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-anB_tOi1QR5J96jnBnE0Q6hhr4cf-TC
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import lightgbm as lgb

from scipy.stats import uniform, randint
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PowerTransformer, LabelEncoder
from sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error

# from google.colab import drive

# drive.mount('/content/drive')

# import data
# training data, data in some columns (2,4) is mix-typed so use unicode
# explicitly assign types to columns
# TODO: change the file path!!!!
t_df = pd.read_csv('source_data/train.csv',
                   index_col=[0],
                   dtype={
                       'Housing Situation': 'unicode',
                       'Work Experience in Current Job [years]': 'unicode'
                   })

p_df = pd.read_csv('source_data/predict.csv',
                   index_col=[0],
                   dtype={
                       'Housing Situation': 'unicode',
                       'Work Experience in Current Job [years]': 'unicode'
                   })

t_df.head()

p_df.head()

rename_map = {
    'Year of Record': 'Year',
    'Housing Situation': 'Housing',
    'Crime Level in the City of Employement': 'Crime',
    'Work Experience in Current Job [years]': 'WorkExp',
    'Satisfation with employer': 'Satisfation',
    'Size of City': 'SoC',
    'University Degree': 'UD',
    'Wears Glasses': 'Glasses',
    'Hair Color': 'Hair',
    'Body Height [cm]': 'Height',
    'Yearly Income in addition to Salary (e.g. Rental Income)': 'AddIn',
    'Total Yearly Income [EUR]': 'Income'
}

t_df.rename(columns=rename_map, inplace=True)
p_df.rename(columns=rename_map, inplace=True)

# see N/A values in train data
t_df.shape[0] - t_df.count()

# see N/A values in predict data
p_df.shape[0] - p_df.count()
"""# Drop training data

drop outliers and na values (any drop operations) before concat training and predict data.
"""

# training data preprocessing before concat with predict data

# randomly drop training data: 70%
# otherwise memory overflow
# t_df = t_df.sample(frac = 0.7, random_state = None)

# training data sort by income
t_df.sort_values(by=['Income'], inplace=True)

# drop duplicates
t_df = t_df.drop_duplicates()

# drop (t_df['Year'] < 2000) & (t_df['Income'] > 400000)
t_df.drop(t_df[(t_df['Year'] < 2000) & (t_df['Income'] > 400000)].index,
          inplace=True)

# # drop Income > 1000000
# t_df.drop(t_df[t_df['Income'] > 1000000].index, inplace=True)

# # drop Crime >= 200
# t_df.drop(t_df[t_df['Crime'] >= 200].index, inplace=True)

# # drop Age >= 110
# t_df.drop(t_df[t_df['Age'] >= 110].index, inplace=True)

# # drop Country '0'
# t_df.drop(t_df[t_df['Country'] == '0'].index, inplace=True)

# rid of N/A or invalid rows for a column (small amount of them)
t_df.dropna(subset=['Year'], inplace=True)
t_df.dropna(subset=['Profession'], inplace=True)
t_df.dropna(subset=['Country'], inplace=True)
t_df.drop(t_df[t_df['WorkExp'] == '#NUM!'].index, inplace=True)
# t_df.drop(t_df[t_df['Gender'] == '0'].index, inplace=True)

# shift p_df's index and t_df's index
t_row, _ = t_df.shape
p_row, _ = p_df.shape

t_df.index = range(1, t_row + 1)
p_df.index = range(t_row + 1, t_row + p_row + 1)

t_df.head()

p_df.head()

# after deletion in on train data
# combine the two dataframes together

total_df = pd.concat([t_df, p_df])

total_row, _ = total_df.shape

print('t_row=', t_row)
print('p_row=', p_row)
print('total_row=', total_row)
"""# Data Preprocessing - Numeric Value"""

########################################### Start with Year

# fillna
total_df['Year'].fillna(1980, inplace=True)

# use 1% sample to plot
total_df[:t_row].sample(frac=0.01, random_state=None).plot.scatter(x='Year',
                                                                   y='Income')

# devide into 2 columns according to the 1990 boundary
total_df['Year_B1990'] = total_df['Year'].map(
    lambda y: y if y <= 1990 else 0.01).astype('float32')
total_df['Year_A1990'] = total_df['Year'].map(
    lambda y: y if y > 1990 else 0.01).astype('float32')

total_df['Year'].astype('int16')

total_df.head()

total_df[(total_df['Year_B1990'] > 0.02) & (total_df['Income'] != 0)].sample(
    frac=0.01, random_state=None).plot.scatter(x='Year_B1990', y='Income')

total_df[(total_df['Year_A1990'] > 0.02) & (total_df['Income'] != 0)].sample(
    frac=0.01, random_state=None).plot.scatter(x='Year_A1990', y='Income')

# 50 ^ 3 * 0.8 = 100,000
# 80 ^ 3 * 0.8 = 409,600
# do a pow of 3 on year

total_df['Year_Resized'] = total_df['Year'].map(lambda y:
                                                (y - 1939)**3).astype('int16')

# freq encode
fq_encode = total_df['Year_Resized'].value_counts(normalize=True).to_dict()
total_df['Year_Resized_F'] = total_df['Year_Resized'].map(fq_encode).astype(
    'float32')

fq_encode = total_df['Year_A1990'].value_counts(normalize=True).to_dict()
total_df['Year_A1990_F'] = total_df['Year_A1990'].map(fq_encode).astype(
    'float32')

fq_encode = total_df['Year_B1990'].value_counts(normalize=True).to_dict()
total_df['Year_B1990_F'] = total_df['Year_B1990'].map(fq_encode).astype(
    'float32')

total_df.head()
########################################### Done with Year

########################################### Start with Housing

# count
total_df['Housing'].value_counts().sort_index()

# Housing - one hot encoding and repale the original column with frequency

# TODO: deal with na and 0 values

dummies = pd.get_dummies(total_df['Housing'], prefix='HS')

total_df = total_df.join(dummies)
fq_encode = total_df['Housing'].value_counts(normalize=True).to_dict()
total_df['Housing_F'] = total_df['Housing'].map(fq_encode).astype('float32')
total_df.drop('Housing', axis=1, inplace=True)

total_df.head()
########################################### Done with Housing

########################################### Start with Crime
# count
total_df['Crime'].value_counts().sort_index()

total_df[(total_df['Income'] != 0)].sample(
    frac=0.01, random_state=None).plot.scatter(x='Crime', y='Income')

total_df[(total_df['Income'] < 10000) & (total_df['Income'] != 0) &
         (total_df['Crime'] >= 25) & (total_df['Crime'] <= 150)].sample(
             frac=0.01, random_state=None).plot.scatter(x='Crime', y='Income')

# frewuency encode
fq_encode = total_df['Crime'].value_counts(normalize=True).to_dict()
total_df['Crime_F'] = total_df['Crime'].map(fq_encode).astype('float32')

# total_df.drop('Crime', axis = 1, inplace = True)
total_df['Crime'] = total_df['Crime'].astype('int32')

total_df.columns
########################################### Done with Crime

########################################### Start with WorkExp
# count
total_df['WorkExp'].value_counts().sort_index()

# 'fillna'
total_df['WorkExp'] = total_df['WorkExp'].map(lambda we: 0
                                              if we == '#NUM!' else we)

# dtype to float
total_df['WorkExp'] = total_df['WorkExp'].astype('float32')

# plot
total_df[(total_df['Income'] != 0)].sample(
    frac=0.01, random_state=None).plot.scatter(x='WorkExp', y='Income')

# count
total_df['WorkExp'].value_counts().sort_index()

# before powertransform make it strictly possitive
total_df['WorkExp'] = total_df['WorkExp'].map(lambda we: we + 0.00001).astype(
    'float32')

# freq encoding
fq_encode = total_df['WorkExp'].value_counts(normalize=True).to_dict()
total_df['WorkExp_F'] = total_df['WorkExp'].map(fq_encode).astype('float32')

# perform powertransform
PT = PowerTransformer(method='box-cox', standardize=True)
total_df['WorkExp_T'] = PT.fit_transform(total_df[['WorkExp'
                                                   ]]).astype('float32')

# perform a inverse
total_df['WorkExp_T'] = total_df['WorkExp_T'].map(
    lambda wet: np.abs(wet)).astype('float32')

total_df[(total_df['Income'] != 0)].sample(
    frac=0.01, random_state=None).plot.scatter(x='WorkExp_T', y='Income')

total_df.columns
########################################### Done with WorkExp

########################################### Start with SoC
# value counts
total_df['SoC'].value_counts().sort_index()

# freq encoding
fq_encode = total_df['SoC'].value_counts(normalize=True).to_dict()
total_df['SoC_F'] = total_df['SoC'].map(fq_encode).astype('float32')

# plot SoC < 1e7
total_df[(total_df['Income'] != 0) & (total_df['SoC'] < 1e7)].sample(
    frac=0.01, random_state=None).plot.scatter(x='SoC', y='Income')

# plot SoC > 1e7
total_df[(total_df['Income'] != 0) & (total_df['SoC'] >= 1e7)].sample(
    frac=0.01, random_state=None).plot.scatter(x='SoC', y='Income')

# devide into 2 parts

total_df['SoC_1e7_1'] = total_df['SoC'].map(
    lambda soc: soc if soc < 1e7 else 0.1).astype('float32')
total_df['SoC_1e7_2'] = total_df['SoC'].map(
    lambda soc: soc if soc >= 1e7 else 0.1).astype('float32')

# total_data_df.drop('SoC', axis = 1, inplace = True)
# TODO: transform

total_df.columns
########################################### End with SoC

########################################### Start with Height
total_df['Height'].value_counts().sort_index()

# plot height
total_df[(total_df['Income'] != 0)].sample(
    frac=0.01, random_state=None).plot.scatter(x='Height', y='Income')

# freq encoding
fq_encode = total_df['Height'].value_counts(normalize=True).to_dict()
total_df['Height_F'] = total_df['Height'].map(fq_encode).astype('float32')

# box-cos transform
PT = PowerTransformer(method='box-cox', standardize=True)
total_df['Height_T'] = PT.fit_transform(total_df[['Height']])

# perform a inverse
total_df['Height_T'] = total_df['Height_T'].map(lambda h: np.abs(h)).astype(
    'float32')

total_df['Height'] = total_df['Height'].astype('int16')

total_df[(total_df['Income'] != 0)].sample(
    frac=0.01, random_state=None).plot.scatter(x='Height_T', y='Income')

total_df.columns
########################################### End with Height

########################################### Start with AddIn
# count
total_df['AddIn'].value_counts().sort_index()

# deal with EUR
total_df['AddIn'] = (
    total_df['AddIn'].map(lambda ai: ai[:-4])).astype('float32')

# freq encoding
fq_encode = total_df['AddIn'].value_counts(normalize=True).to_dict()
total_df['AddIn_F'] = total_df['AddIn'].map(fq_encode).astype('float32')

# devide
total_df['AddIn_0'] = total_df['AddIn'].map(lambda ai: 1 if ai < 0.1 else 0)

# plot
total_df[(total_df['Income'] != 0)].sample(
    frac=0.01, random_state=None).plot.scatter(x='AddIn', y='Income')

total_df.columns
########################################### End with AddIn
"""# One Hot Encoding

One Hot Encoding takes great time on large dataset and may cause `tcmalloc: large alloc` and stop the jupyter kernel.

So do it at the end of preprocessing, after the plot and preprocessing of the numberic values.
"""

########################################### Start with Satisfation
# count
total_df['Satisfation'].value_counts().sort_index()

# fillna
total_df['Satisfation'].fillna('Average', inplace=True)

dummies = pd.get_dummies(total_df['Satisfation'], prefix='Satis')
total_df = total_df.join(dummies)

# freq encoding
fq_encode = total_df['Satisfation'].value_counts(normalize=True).to_dict()
total_df['Satisfation_F'] = total_df['Satisfation'].map(fq_encode).astype(
    'float32')

total_df.drop('Satisfation', axis=1, inplace=True)

total_df.columns
########################################### Done with Satisfation

########################################### Start with Gender
# count
total_df['Gender'].value_counts().sort_index()

# fillna
total_df['Gender'].fillna('unknown', inplace=True)

# frequency encoding
fq_encode = total_df['Gender'].value_counts(normalize=True).to_dict()
total_df['Gender_F'] = total_df['Gender'].map(fq_encode).astype('float32')

# DEPRECATED
# # f => female
# total_df['Gender'] = total_df['Gender'].map(lambda g: 'female' if g == 'f' else g)
# # 0 => unknown
# total_df['Gender'] = total_df['Gender'].map(lambda g: 'unknown' if g == '0' else g)
# DEPRECATED

# TODO: unknown values

# one hot encoding
dummies = pd.get_dummies(total_df['Gender'], prefix='G')
total_df = total_df.join(dummies)

total_df.drop('Gender', axis=1, inplace=True)

total_df.columns
########################################### Done with Gender

########################################### Start with Age
# count
total_df['Age'].value_counts().sort_index()

# freq encoding
fq_encode = total_df['Age'].value_counts(normalize=True).to_dict()
total_df['Age_F'] = total_df['Age'].map(fq_encode).astype('float32')

# plot
total_df[(total_df['Income'] != 0) & (total_df['Age'] >= 80) &
         (total_df['Age'] < 110)].sample(
             frac=0.01, random_state=None).plot.scatter(x='Age', y='Income')

# divide
total_df[(total_df['Income'] != 0)].sample(
    frac=0.01, random_state=None).plot.scatter(x='Age', y='Income')

total_df.columns
########################################### Done with Age

########################################### Start with Country
# count
total_df['Country'].value_counts().sort_index()

# fillna padding
total_df['Country'].fillna(method='pad', inplace=True)

# freq encoding
fq_encode = total_df['Country'].value_counts(normalize=True).to_dict()
total_df['Country_F'] = total_df['Country'].map(fq_encode).astype('float32')

# one hot encoding
dummies = pd.get_dummies(total_df['Country'], prefix='CTR')
total_df = total_df.join(dummies)

# drop the original column
total_df.drop('Country', axis=1, inplace=True)

total_df.columns
########################################### End with Country

########################################### Start with Profession
total_df['Profession'].value_counts().sort_index()

# fillna
total_df['Profession'].fillna('unknown', inplace=True)

# freq encoding
fq_encode = total_df['Profession'].value_counts(normalize=True).to_dict()
total_df['Profession_F'] = total_df['Profession'].map(fq_encode).astype(
    'float32')

# one hot encoding
dummies = pd.get_dummies(total_df['Profession'], prefix='PRO')
total_df = total_df.join(dummies)

# drop the original column
total_df.drop('Profession', axis=1, inplace=True)

total_df.columns
########################################### End with Profession

########################################### Start with UD
# count
total_df['UD'].value_counts().sort_index()

# fillna padding
total_df['UD'].fillna(method='pad', inplace=True)

# '0' as 'No'
total_df['UD'] = total_df['UD'].map(lambda ud: ud if ud != '0' else 'No')
# TODO classification fill na / 0 value

# freq encoding
fq_encode = total_df['UD'].value_counts(normalize=True).to_dict()
total_df['UD_F'] = total_df['UD'].map(fq_encode).astype('float32')

# one hot encoding
dummies = pd.get_dummies(total_df['UD'], prefix='UD')
total_df = total_df.join(dummies)

# drop the original column
total_df.drop('UD', axis=1, inplace=True)

total_df.columns
########################################### End with UD

########################################### Start with Glasses
# count
total_df['Glasses'].value_counts().sort_index()

# freq encoding
fq_encode = total_df['Glasses'].value_counts(normalize=True).to_dict()
total_df['Glasses_F'] = total_df['Glasses'].map(fq_encode).astype('float32')

# one hot encoding
dummies = pd.get_dummies(total_df['Glasses'], prefix='GL')
total_df = total_df.join(dummies)

# drop the original column
total_df.drop('Glasses', axis=1, inplace=True)

total_df.columns
########################################### End with UD

########################################### Start with Hair
# count
total_df['Hair'].value_counts().sort_index()

# fillna padding
total_df['Hair'].fillna(method='pad', inplace=True)
# '0' as 'Unknown'
total_df['Hair'] = total_df['Hair'].map(lambda hc: hc
                                        if hc != '0' else 'Unknown')
# TODO classification fill na / 0 value

# freq encoding
fq_encode = total_df['Hair'].value_counts(normalize=True).to_dict()
total_df['Hair_F'] = total_df['Hair'].map(fq_encode).astype('float32')

# one hot encoding
dummies = pd.get_dummies(total_df['Hair'], prefix='Hair')
total_df = total_df.join(dummies)

# drop the original column
total_df.drop('Hair', axis=1, inplace=True)

total_df.columns
########################################### Start with Hair
"""# Magical Timeblock Frequency Encoding (Division)

creating new features
"""


def create_cat_con(df, cats, cons, normalize=True):
    for i, cat in enumerate(cats):
        for j, con in enumerate(cons):
            new_col = cat + '_' + con
            print('timeblock frequency encoding:', new_col)
            df[new_col] = df[cat].astype(str) + '_' + df[con].astype(str)
            df[new_col] = df[con] / df[cat]
            df[new_col].astype('float32', inplace=True)
    return df


cats = [
    'Year_Resized_F', 'Year_B1990_F', 'Year_A1990_F', 'Housing_F', 'WorkExp_F',
    'Satisfation_F', 'Gender_F', 'Age_F', 'Country_F', 'Profession_F', 'UD_F',
    'Glasses_F', 'Hair_F'
]
cons = ['Crime_F', 'SoC_F', 'AddIn_F']

total_df = create_cat_con(total_df, cats, cons)

print('create_cat_con OK!!!!!!')
"""# Training Phase"""

total_no_income = total_df.drop('Income', axis=1, inplace=False)

X = total_no_income[:t_row]

X_with_addin = X[X['AddIn'] > 0]
X_without_addin = X[X['AddIn_0'] == 1]

y_ttl = total_df[:t_row]
y_with_addin = y_ttl[y_ttl['AddIn'] > 0]['Income']
y_without_addin = y_ttl[y_ttl['AddIn_0'] == 1]['Income']

# lgb parameters
params = {
    'max_depth': 25,
    'num_leaves': 100,
    'bagging_fraction': 0.8
    'learning_rate': 0.1,
    "boosting": "gbdt",
    "metric": 'mae',
    "verbosity": -1,
}

# kfold with addin

kfold = KFold(n_splits=5, random_state=None, shuffle=True)
results_arr_with_addin = []
iii = 0
for train_index, test_index in kfold.split(X_with_addin):
    print(iii, ':')
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X_with_addin.values[train_index], X_with_addin.values[
        test_index]
    y_train, y_test = y_with_addin.values[train_index], y_with_addin.values[
        test_index]

    # lgb OVERFLOWED
    trn_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_test, label=y_test)
    model = lgb.train(params,
                      trn_data,
                      10000,
                      valid_sets=[trn_data, val_data],
                      verbose_eval=1000,
                      early_stopping_rounds=300)

    # simple linear
    # model = LinearRegression()
    # model.fit(X_train, y_train)
    # score = model.score(X_test, y_test)

    # calculate val
    test_val = model.predict(X_test)
    score = mean_absolute_error(y_test, test_val)

    print('score:')
    print(score)
    results_arr_with_addin.append((model, score))
    iii += 1

# kfold without addin

kfold = KFold(n_splits=5, random_state=None, shuffle=True)
results_arr_without_addin = []
iii = 0
for train_index, test_index in kfold.split(X_without_addin):
    print(iii, ':')
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X_without_addin.values[
        train_index], X_without_addin.values[test_index]
    y_train, y_test = y_without_addin.values[
        train_index], y_without_addin.values[test_index]

    # lgb OVERFLOWED
    trn_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_test, label=y_test)
    model = lgb.train(params,
                      trn_data,
                      10000,
                      valid_sets=[trn_data, val_data],
                      verbose_eval=1000,
                      early_stopping_rounds=300)

    # simple linear
    # model = LinearRegression()
    # model.fit(X_train, y_train)
    # score = model.score(X_test, y_test)

    # calculate val
    test_val = model.predict(X_test)
    score = mean_absolute_error(y_test, test_val)

    print('score:')
    print(score)
    results_arr_without_addin.append((model, score))
    iii += 1
"""# Output Data"""

# X_predict
X_predict = total_no_income[t_row:]

Xp_with_addin = X_predict[X_predict['AddIn'] > 0]
Xp_without_addin = X_predict[X_predict['AddIn_0'] == 1]

# pick one result
best_one, best_score = results_arr_with_addin[0]
for sets in results_arr_with_addin:
    m, s = sets
    if s < best_score:
        best_score = s
        best_one = m

Xp_with_addin['Income'] = best_one.predict(Xp_with_addin)

best_one, best_score = results_arr_without_addin[0]
for sets in results_arr_with_addin:
    m, s = sets
    if s < best_score:
        best_score = s
        best_one = m

Xp_without_addin['Income'] = best_one.predict(Xp_without_addin)

# concat and sort
Xp_with_prediction = pd.concat([Xp_with_addin, Xp_without_addin])
Xp_with_prediction.sort_index(inplace=True)

sub_df = pd.DataFrame({
    'Instance': range(1, p_row + 1),
    'Total Yearly Income [EUR]': Xp_with_prediction['Income']
})

# write file
from time import strftime

sub_df.to_csv(strftime('%Y_%m_%d__%H_%M_%S') + '.csv', index=False)
